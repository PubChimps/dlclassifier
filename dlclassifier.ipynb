{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import base64\n",
    "import github\n",
    "import zipfile\n",
    "import operator\n",
    "import requests\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from timeit import Timer\n",
    "from collections import *\n",
    "from keras.optimizers import SGD\n",
    "from github import GithubException\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__) #this project using TensorFlow version 2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetlangs = ['.ipynb']\n",
    "def parsenb(dataset):\n",
    "    parseddataset = []\n",
    "   \n",
    "    for i in range(len(dataset)):\n",
    "        code = ''\n",
    "        try:\n",
    "            parsednb = (json.loads(dataset[i][1]))\n",
    "        except:\n",
    "            print(str(i) + ' ' + str(sys.exc_info()))\n",
    "        for j in range(len(parsednb['cells'])):\n",
    "            if parsednb['cells'][j]['cell_type'] == 'code':\n",
    "                code = code + ''.join(parsednb['cells'][j]['source'])\n",
    "        parseddataset.append([code, dataset[i][2]])\n",
    "    \n",
    "    for i in range(len(parseddataset)):\n",
    "        parseddataset[i][0] = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', parseddataset[i][0]).replace('\\n', ' ')\n",
    "    return parseddataset\n",
    "\n",
    "def download_directory(repository, path, framework):\n",
    "    global dataset\n",
    "    try:\n",
    "        contents = repository.get_contents(path)\n",
    "        for content in contents:\n",
    "            if content.type == 'dir':\n",
    "                download_directory(repository, content.path, framework)\n",
    "            else:\n",
    "                if content.content:\n",
    "                    if len(str(content.name).split(\".\")) == 2:\n",
    "                        if any(substring == (\".\" + str(content.name).split(\".\")[1]) for substring in targetlangs):                           \n",
    "                            try:\n",
    "                                dataset.append([repository,str(base64.b64decode(content.content),'utf-8'),framework])\n",
    "                            except (GithubException, IOError) as exc:\n",
    "                                print('Error processing %s: %s', content.path, exc)\n",
    "    except (GithubException, IOError) as exc:\n",
    "        print(\"error in dir \")\n",
    "        \n",
    "def fleiss_kappa(lists, classes):\n",
    "    n = len(lists)\n",
    "    N = len(lists[0])\n",
    "    k = len(classes)\n",
    "    \n",
    "    nij = np.zeros([N,k])\n",
    "        \n",
    "    for i in range(len(lists)):\n",
    "        for j in range(len(lists[i])):\n",
    "            nij[j][classes.index(lists[i][j])] += 1 \n",
    "    \n",
    "    p = np.sum(nij, axis=0) / (N * n)\n",
    "    P = (np.sum(nij * nij, axis=1) - n) / (n * (n - 1))\n",
    "    Pbar = np.sum(P) / N\n",
    "    PbarE = np.sum(p * p)\n",
    "\n",
    "    return (Pbar - PbarE) / (1 - PbarE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Collect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authenticate github\n",
    "login = '****'\n",
    "password = '****'\n",
    "\n",
    "g = github.Github(login,password)\n",
    "\n",
    "#get list of repos referecencing deep learning frameworks\n",
    "tensorflowrepos = list(g.search_repositories('tensorflow language:\"Jupyter Notebook\" created:\"2019-01-01 .. 2019-01-31\"'))\n",
    "pytorchrepos = list(g.search_repositories('pytorch language:\"Jupyter Notebook\" created:\"2019-01-01 .. 2019-01-31\"'))\n",
    "\n",
    "#github limits the number of returns for a search call, \n",
    "#add to list of repos by making another call in a different 'created:' range\n",
    "#tensorflowrepos += list(g.search_repositories('tensorflow language:\"Jupyter Notebook\" created:\"2019-02-01 .. 2019-02-28\"'))\n",
    "\n",
    "#label repos\n",
    "repolist = []\n",
    "for i in pytorchrepos:\n",
    "    username,reponame = str(i).split('\"')[1].split('/')\n",
    "    user = g.get_user(username)\n",
    "    repolist.append([user.get_repo(reponame),'pytorch'])\n",
    "for i in tensorflowrepos:\n",
    "    username,reponame = str(i).split('\"')[1].split('/')\n",
    "    user = g.get_user(username)\n",
    "    repolist.append([user.get_repo(reponame),'tensorflow'])\n",
    "    \n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download notebooks, check limit with g.get_rate_limit() and rerun when available\n",
    "last = 0\n",
    "for i in range(len(repolist[last:])):\n",
    "    last = last + i\n",
    "    download_directory(repolist[i][0],'',repolist[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1* - Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you'd prefer to load a dataset rather than build one, run this cell\n",
    "url = 'https://github.com/PubChimps/dlclassifier/blob/master/dlzip.npz.zip?raw=true'\n",
    "r = requests.get(url)\n",
    "open('./dlzip.npz.zip', 'wb').write(r.content)\n",
    "zippedfile = zipfile.ZipFile('./dlzip.npz.zip')\n",
    "zippedfile.extractall()\n",
    "dataset = np.load('dlzip.npz', allow_pickle = True)\n",
    "dataset = dataset.f.arr_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "labeledlines = []\n",
    "ignore_words = ['?', ',', 'tensorflow', 'tf', 'TensorFlow', 'pytorch']\n",
    "\n",
    "\n",
    "for line in dataset:\n",
    "    text = str(re.split(r'[.,]', line[0])).replace(\"'\",\"\").replace('[','')\n",
    "    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n",
    "    w = nltk.word_tokenize(text)\n",
    "    w = [ele for ele in w if ele not in ignore_words]\n",
    "    words.extend(w)\n",
    "    labeledlines.append([w, line[1]])\n",
    "    \n",
    "words = list(set(words))\n",
    "\n",
    "data = []\n",
    "for line in range(len(dataset)):\n",
    "    #this loop takes a while (>20 minutes)\n",
    "    #track progress by uncommenting the line below and comparing it to len(dataset)\n",
    "    #print(line)\n",
    "    bag = []\n",
    "    code = dataset[line][0]\n",
    "    for w in words:\n",
    "        bag.append(code.count(w)) \n",
    "        \n",
    "    classes = [0]\n",
    "    if labeledlines[line][1] == 'tensorflow':\n",
    "        classes[0] = 1\n",
    "    elif labeledlines[line][1] == 'pytorch':\n",
    "        classes[0] = 0\n",
    "\n",
    "    data.append([bag,classes])\n",
    "    \n",
    "trainset = data[:9180]\n",
    "testset = data[9180:]\n",
    "x_train = np.array([row[0] for row in trainset])\n",
    "y_train = np.array([row[1] for row in trainset])\n",
    "x_test = np.array([row[0] for row in testset])\n",
    "y_test = np.array([row[1] for row in testset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Dense(1024, activation='relu', input_dim= len(words)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=500,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = dataset[:,0]\n",
    "stopwords = ['tf', 'the', 'torch', 'keras', 'tensor', 'tensorflow']\n",
    "for i in range(len(code)):\n",
    "    code [i] = re.sub(r'\\b\\w{1,1}\\b', '', code[i])\n",
    "    for word in stopwords:\n",
    "        if word in code[i]:\n",
    "            code[i] = code[i].replace(word,'')\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(code)\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "maxlen = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_train = code[:9180]\n",
    "code_test = code[9180:]\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(code_train)\n",
    "X_test = tokenizer.texts_to_sequences(code_test)\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embmodel = Sequential()\n",
    "embmodel.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "embmodel.add(layers.Flatten())\n",
    "embmodel.add(layers.Dense(10, activation='relu'))\n",
    "embmodel.add(layers.Dense(1, activation='sigmoid'))\n",
    "embmodel.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "embmodel.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = embmodel.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = embmodel.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenetmodel = Sequential()\n",
    "lenetmodel.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "\n",
    "lenetmodel.add(layers.Conv1D(filters=6, kernel_size=(3), activation='relu'))\n",
    "lenetmodel.add(layers.AveragePooling1D())\n",
    "\n",
    "lenetmodel.add(layers.Conv1D(filters=16, kernel_size=(3), activation='relu'))\n",
    "lenetmodel.add(layers.AveragePooling1D())\n",
    "\n",
    "lenetmodel.add(layers.Flatten())\n",
    "\n",
    "lenetmodel.add(layers.Dense(units=120, activation='relu'))\n",
    "\n",
    "lenetmodel.add(layers.Dense(units=84, activation='relu'))\n",
    "\n",
    "lenetmodel.add(layers.Dense(units=1, activation = 'sigmoid'))\n",
    "lenetmodel.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lenetmodel.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = lenetmodel.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = lenetmodel.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = Sequential()\n",
    "vgg.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "\n",
    "vgg.add(layers.Conv1D(filters=64, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.Conv1D(filters=64, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "vgg.add(layers.Conv1D(filters=128, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.Conv1D(filters=128, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "vgg.add(layers.Conv1D(filters=256, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.Conv1D(filters=256, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "vgg.add(layers.Conv1D(filters=512, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.Conv1D(filters=512, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "vgg.add(layers.Conv1D(filters=512, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.Conv1D(filters=512, kernel_size=(3), activation='relu', padding='same'))\n",
    "vgg.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "vgg.add(layers.Flatten())\n",
    "\n",
    "vgg.add(layers.Dense(units=120, activation='relu'))\n",
    "\n",
    "vgg.add(layers.Dense(units=84, activation='relu'))\n",
    "\n",
    "vgg.add(layers.Dense(units=1, activation = 'sigmoid'))\n",
    "vgg.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "vgg.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = vgg.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = vgg.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Timer(lambda: mlpmodel.fit(x_train, y_train,\n",
    "                    epochs=1,\n",
    "                    verbose=False,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    batch_size=10))\n",
    "\n",
    "print('NN number of parameters ' + str(mlpmodel.count_params()))\n",
    "loss, mlpaccuracy = mlpmodel.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"NN Training Accuracy: {:.4f}\".format(mlpaccuracy))\n",
    "loss, mlptestaccuracy = mlpmodel.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"NN Testing Accuracy:  {:.4f}\".format(mlptestaccuracy))\n",
    "print('NN epoch training time ' + str(t.timeit(number=1)) + '\\n\\n')\n",
    "print()\n",
    "mlppreds = mlpmodel.predict_classes(x_test)\n",
    "\n",
    "t = Timer(lambda: embmodel.fit(X_train, y_train,\n",
    "                    epochs=1,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10))\n",
    "\n",
    "print('Embedded Model number of parameters ' + str(embmodel.count_params()))\n",
    "loss, emdtrainacc = embmodel.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Embedded Model Training Accuracy: {:.4f}\".format(emdtrainacc))\n",
    "loss, emdtestacc = embmodel.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Embedded Model Testing Accuracy:  {:.4f}\".format(emdtestacc))\n",
    "print('Embedded Model epoch training time ' + str(t.timeit(number=1)) + '\\n\\n')\n",
    "\n",
    "emdpreds = embmodel.predict_classes(X_test)\n",
    "\n",
    "t = Timer(lambda: lenetmodel.fit(X_train, y_train,\n",
    "                    epochs=1,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10))\n",
    "print('Text CNN number of parameters ' + str(lenetmodel.count_params()))\n",
    "loss, cnntrainaccuracy = lenetmodel.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Text CNN Training Accuracy: {:.4f}\".format(cnntrainaccuracy))\n",
    "loss, cnntestaccuracy = lenetmodel.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Text CNN Testing Accuracy:  {:.4f}\".format(cnntestaccuracy))\n",
    "print('Text CNN training time ' + str(t.timeit(number=1)) + '\\n\\n')\n",
    "\n",
    "textcnnpreds = lenetmodel.predict_classes(X_test)\n",
    "\n",
    "t = Timer(lambda: vgg.fit(X_train, y_train,\n",
    "                    epochs=1,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10))\n",
    "print('Text VGG number of parameters ' + str(vgg.count_params()))\n",
    "loss, vggtrainaccuracy = vgg.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Text VGG Training Accuracy: {:.4f}\".format(vggtrainaccuracy))\n",
    "loss, vggtestaccuracy = vgg.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Text VGG Testing Accuracy:  {:.4f}\".format(vggtestaccuracy))\n",
    "print('Text VGG training time ' + str(t.timeit(number=1)) + '\\n\\n')\n",
    "vggpreds = vgg.predict_classes(X_test)\n",
    "\n",
    "\n",
    "preds = [mlppreds, emdpreds, textcnnpreds, vggpreds ]\n",
    "print(\"Fleiss' Kappa \" + str(fleiss_kappa(preds, [0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fleiss' Kappa \" + str(fleiss_kappa(preds, [0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
